# CSU22012 - Algorithms and Data Structures II Notes

## ğŸ“š Overview

These are the notes I made for CSU22012 â€“ Algorithms and Data Structures II at Trinity College Dublin (2022-2023). They cover the majority of the course syllabus, including a wide range of algorithms, data structures, and design paradigms discussed throughout the term. These notes are made to help in exam preparation and provide a simple way to understand key concepts in the module.

## Topic 1: Sorting algorithms

Here are sorting algorithms we covered. ğŸš€

### 1.1 Insertion sort

- ğŸ§ Logic: Iterate through the list, insert each element into its correct position in the sorted section.
- â±ï¸ Best case: O(n)
- â±ï¸ Worst case: O(n^2)
- âœ… Stable: Yes
- âœ… In-place: Yes
- ğŸ’ª Algorithm type: Incremental
- ğŸ’» Pseudocode:

```
      for i in range(1, len(arr)):
    	key = arr[i]
    	j = i - 1

    	while j >= 0 and key < arr[j]:
    		arr[j+1] = arr[j]
    		j -= 1
    		arr[j+1] = key

```

### 1.2 Bubble sort

- ğŸ§ Logic: Repeatedly swap adjacent elements if they are in the wrong order.
- â±ï¸ Best case: O(n)
- â±ï¸ Worst case: O(n^2)
- âœ… Stable: Yes
- âœ… In-place: Yes
- ğŸ’ª Algorithm type: Incremental
- ğŸ’» Pseudocode:

```
    for i in range(len(arr)-1):
	    for j in range(len(arr)-i-1):
    	    if arr[j] > arr[j+1]:
    		    arr[j], arr[j+1] = arr[j+1], arr[j]

```

### 1.3 Selection sort

- ğŸ§ Logic: Select the smallest element in the unsorted section and swap it with the first unsorted element.
- â±ï¸ Best case: O(n^2)
- â±ï¸ Worst case: O(n^2)
- âŒ Stable: No
- âœ… In-place: Yes
- ğŸ’ª Algorithm type: Incremental
- ğŸ’» Pseudocode:

```
for i in range(len(arr)):
	min_idx = i for j in range(i+1, len(arr)):
		if arr[j] < arr[min_idx]:
			min_idx = j arr[i], arr[min_idx] = arr[min_idx], arr[i]

```

### 1.4 Merge sort

- ğŸ§ Logic: Divide the list into halves, recursively sort the halves, and merge them.
- â±ï¸ Best case: O(n * log(n))
- â±ï¸ Worst case: O(n * log(n))
- âœ… Stable: Yes
- âŒ In-place: No (requires additional memory)
- ğŸ’ª Algorithm type: Divide and conquer
- ğŸ’» Pseudocode:

```
def merge_sort(arr):
	if len(arr) <= 1: return arr

	mid = len(arr) // 2

	left = merge_sort(arr[:mid])
	right = merge_sort(arr[mid:])

	return merge(left, right)

def merge(left, right):
	result = []
	i, j = 0

	while i < len(left) and j < len(right):
		if left[i] < right[j]:
			result.append(left[i])
			i += 1
		else:
			result.append(right[j])
			j += 1

	result += left[i:]
	result += right[j:]

	return result

```

### 1.5 Quick sort

- ğŸ§ Logic: Select a pivot, partition the list around the pivot, and recursively sort the partitions.
- â±ï¸ Best case: O(n * log(n))
- â±ï¸ Worst case: O(n^2) (rarely occurs with bad pivot selection)
- âœ… Stable: No
- âœ… In-place: Yes
- ğŸ’ª Algorithm type: Divide and conquer
- ğŸ’» Pseudocode:

```
def quick_sort(arr, low, high):
    if low < high:
        pi = partition(arr, low, high)
        quick_sort(arr, low, pi - 1)
        quick_sort(arr, pi + 1, high)

def partition(arr, low, high):
    pivot = arr[high]
    i = low - 1
    for j in range(low, high):
        if arr[j] < pivot:
            i += 1
            arr[i], arr[j] = arr[j], arr[i]
    arr[i+1], arr[high] = arr[high], arr[i+1]
    return i+1

```

### 1.6 Shell sort

- ğŸ§ Logic: Sort elements at specific intervals, reducing the interval until it's 1 (Insertion sort).
- â±ï¸ Best case: O(n * log(n))
- â±ï¸ Worst case: O(n^(3/2)) (depends on the gap sequence)
- âŒ Stable: No
- âœ… In-place: Yes
- ğŸ’ª Algorithm type: Incremental
- ğŸ’» Pseudocode:

```
def shell_sort(arr):
    n = len(arr)
    gap = n // 2
    while gap > 0:
        for i in range(gap, n):
            temp = arr[i]
            j = i
            while j >= gap and arr[j - gap] > temp:
                arr[j] = arr[j - gap]
                j -= gap
            arr[j] = temp
        gap //= 2

```

## Topic 2: Graphs

- ğŸ¤” A graph is a collection of nodes (vertices) connected by edges.
- ğŸ‘¨â€ğŸ“ **Pro Tip:** Refer to the graph notes from Discrete Maths, specially for MSTs.

### 2.1 Using Graphs

#### 2.1.1 Graph ADT

- ğŸ§‘â€ğŸ’» Abstract Data Type representing a graph, typically including methods for adding vertices, edges, and querying neighbours.

#### 2.1.2 Array-based Representations

- ğŸ“Š Two common ways to represent a graph using arrays.

#### 2.1.2.1 Adjacency Matrix

- 2D array with rows/columns representing vertices and elements indicating edge presence/weight.
- ğŸ’» Pseudocode for checking if there is an edge between two vertices:

```
if adjacency_matrix[v1][v2] != 0:
    ## there is an edge between v1 and v2

```

#### 2.1.2.2 Adjacency List

- Array of lists, each list representing a vertex's neighbors.
- ğŸ’» Pseudocode for checking if there is an edge between two vertices:

```
if v2 in adjacency_list[v1]:
    ## there is an edge between v1 and v2

```

### 2.2 Traversal Algorithms

#### 2.2.1 DFS (Depth-First Search)

- Algorithm for traversing or searching graphs, visiting children of a node before visiting its siblings.
- â±ï¸ Time complexity: O(V+E)
- ğŸ’¾ Space complexity: O(V)
- ğŸ’» Pseudocode:

```
def dfs(graph, start, visited=None):
    if visited is None:
        visited = set()
    visited.add(start)
    for neighbor in graph[start]:
        if neighbor not in visited:
            dfs(graph, neighbor, visited)

```

- Applications: Pathfinding, Connected components, Topological sorting

#### 2.2.2 BFS (Breadth-First Search)

- Algorithm for traversing or searching graphs, visiting all neighbors of a node before visiting their children.
- â±ï¸ Time complexity: O(V+E)
- ğŸ’¾ Space complexity: O(V)
- ğŸ’» Pseudocode:

```
def bfs(graph, start):
    visited = set()
    queue = [start]
    while queue:
        vertex = queue.pop(0)
        if vertex not in visited:
            visited.add(vertex)
            queue.extend(graph[vertex] - visited)
    return visited

```

- Applications: Shortest path, Connected components

### 2.3 Shortest Path Algorithms

#### 2.3.1 Dijkstra's Algorithm

- Finds the shortest path from a single source to all other vertices in a weighted graph with non-negative weights.
- â±ï¸ Time complexity: O(V^2) or O(V+E*log(V)) with priority queue
- ğŸ’° Greedy algorithm
- âŒ Does not work with negative edge weights
- ğŸ’» Pseudocode:

```
def dijkstra(graph, start):
    dist = {node: float('inf') for node in graph}
    dist[start] = 0
    pq = [(0, start)]
    while pq:
        (cost, node) = heapq.heappop(pq)
        if cost > dist[node]:
            continue
        for neighbor, weight in graph[node].items():
            alt = cost + weight
            if alt < dist[neighbor]:
                dist[neighbor] = alt
                heapq.heappush(pq, (alt, neighbor))
    return dist

```

#### 2.3.2 Bellman-Ford Algorithm

- Finds the shortest path from a single source to all other vertices in a weighted graph, even with negative weights.
- â±ï¸ Time complexity: O(V*E)
- â— Detects negative-weight cycles
- ğŸ’» Pseudocode:

```
    dist = {node: float('inf') for node in graph}
    dist[start] = 0
    for i in range(len(graph) - 1):
        for u, edges in graph.items():
            for v, weight in edges.items():
                if dist[u] + weight < dist[v]:
                    dist[v] = dist[u] + weight
    for u, edges in graph.items():
        for v, weight in edges.items():
            if dist[u] + weight < dist[v]:
                raise ValueError("Negative weight cycle detected")
    return dist

```

### 2.4 Topological Sort

- Linear ordering of vertices in a directed acyclic graph (DAG) such that for every directed edge (u, v), vertex u comes before vertex v.
- â±ï¸ Time complexity: O(V+E)
- ğŸ¦„ Can produce different outputs for the same graph
- ğŸ’» Pseudocode:

```
def topological_sort(graph):
    in_degree = {node: 0 for node in graph}
    for node in graph:
        for neighbor in graph[node]:
            in_degree[neighbor] += 1

    queue = [node for node in in_degree if in_degree[node] == 0]
    result = []
    while queue:
        node = queue.pop(0)
        result.append(node)
        for neighbor in graph[node]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)

    if len(result) != len(graph):
        raise ValueError("Graph has a cycle")
    return result

```

### 2.5 Minimum Spanning Tree Algorithms

#### 2.5.1 Prim's Algorithm

- Finds the minimum spanning tree of an undirected, connected, weighted graph.
- â±ï¸ Time complexity: O(V^2) or O(E*log(V)) with priority queue
- ğŸ’° Greedy algorithm
- ğŸ’» Pseudocode:

```
    mst = set()
    visited = set()
    start_node = next(iter(graph))
    visited.add(start_node)
    edges = [
        (cost, start_node, to)
        for to, cost in graph[start_node].items()
    ]
    heapq.heapify(edges)

    while edges:
        cost, frm, to = heapq.heappop(edges)
        if to not in visited:
            visited.add(to)
            mst.add((frm, to, cost))
            for to_next, cost in graph[to].items():
                if to_next not in visited:
                    heapq.heappush(edges, (cost, to, to_next))

    return mst

```

#### 2.5.2 Kruskal's Algorithm

- Finds the minimum spanning tree of an undirected, connected, weighted graph.
- â±ï¸ Time complexity: O(E*log(V))
- ğŸ’° Greedy algorithm
- ğŸ’» Pseudocode:

```
def kruskal(graph):
    mst = set()
    edges = [
        (cost, frm, to)
        for frm in graph
        for to, cost in graph[frm].items()
    ]
    edges.sort()

    parent = {node: node for node in graph}

    def find(node):
        if parent

```

### 2.6 All Pairs Shortest Path

#### 2.6.1 Floyd-Warshall Algorithm

- Finds the shortest paths between all pairs of vertices in a weighted graph with or without negative edge weights (but no negative cycles).
- â±ï¸ Time complexity: O(V^3)
- ğŸ’» Pseudocode:

```
def floyd_warshall(graph):
    dist = {}
    for i in graph:
        dist[i] = {}
        for j in graph:
            dist[i][j] = float("inf")

    for node in graph:
        dist[node][node] = 0
        for neighbor, weight in graph[node].items():
            dist[node][neighbor] = weight

    for k in graph:
        for i in graph:
            for j in graph:
                if dist[i][k] + dist[k][j] < dist[i][j]:
                    dist[i][j] = dist[i][k] + dist[k][j]

    return dist

```

### 2.7 Single-Pair Shortest Path

#### 2.7.1 Uniform Cost Search

- A search algorithm that expands the node with the lowest path cost.
- â±ï¸ Time complexity: O(b^d) (b: branching factor, d: depth of the solution)
- ğŸ’¾ Space complexity: O(b^d)
- Applications: Pathfinding, AI planning
- ğŸ’» Pseudocode:

```
def uniform_cost_search(graph, start, goal):
    queue = [(0, start)]
    visited = set()
    while queue:
        (cost, node) = heapq.heappop(queue)
        if node == goal:
            return cost
        if node not in visited:
            visited.add(node)
            for neighbor, weight in graph[node].items():
                heapq.heappush(queue, (cost + weight, neighbor))
    return None

```

#### 2.7.2 Greedy Best-First Search

- A search algorithm that expands the node with the lowest heuristic value.
- â±ï¸ Time complexity: O(b^m) (b: branching factor, m: maximum depth of the search space)
- ğŸ’¾ Space complexity: O(b^m)
- Applications: Pathfinding, AI planning, constraint satisfaction problems
- ğŸ’» Pseudocode:

```
def greedy_best_first_search(graph, start, goal, heuristic):
    queue = [(heuristic(start, goal), start)]
    visited = set()
    while queue:
        (cost, node) = heapq.heappop(queue)
        if node == goal:
            return cost
        if node not in visited:
            visited.add(node)
            for neighbor, weight in graph[node].items():
                heapq.heappush(queue, (heuristic(neighbor, goal), neighbor))
    return None

```

â€¼ï¸ **NB** that `heuristic` is a function that estimates the cost from a given node to the goal node. The idea is to use this heuristic to guide the search towards the goal node.

#### 2.7.3 A* Search

- Finds the shortest path between two nodes in a graph, combines the cost function of the Uniform Cost and the heuristic function from the Greedy Best-First to guide the search.
- â±ï¸ Time complexity: O(V+E*log(V)) (depends on the heuristic)

## Topic 3: Recursion

### 3.1 What is recursion

- ğŸ¤” A process in which a function calls itself as a subroutine to solve a problem.
- ğŸ’« In a recursive function, the solution to the base case is provided, and the problem is divided into smaller subproblems.
- ğŸ˜ The base case is the simplest possible case that can be solved directly.

### 3.2 Pros and cons of recursion

- ğŸ‘ Recursion can simplify code and make it easier to understand.
- ğŸ‘ Recursion is often more elegant and concise than iterative solutions.
- ğŸ‘ Recursion can be less efficient than iterative solutions, especially for large problems.
- ğŸ‘ Recursion can be harder to debug and understand for complex problems.
- ğŸ‘ Recursion can cause infinite loops if the base case isn't properly defined.

### 3.3 Tail Recursion

- ğŸš€ A form of recursion where the recursive call is the last operation performed in the function.
- ğŸ“Š The recursive call is optimized by the compiler, which turns it into a loop.
- ğŸ’¡ Why Tail Recursion?
    - Tail recursion is usually more efficient (although more difficult to write) than non-tail recursion.
    - The recursive calls do not need to be added to the call stack: there is only one, the current call, in the stack.
    - It is possible to turn tail recursions into iterative algorithms.

#### 3.3.1 Example of Tail Recursion

- ğŸ§‘â€ğŸ’» Here's an example of a tail-recursive function that calculates the factorial of a given number:

```
function factorial(n, acc = 1) {
  if (n === 0) return acc;
  return factorial(n - 1, acc * n);
}

```

- ğŸ“ The `factorial` function takes two arguments: `n` is the number whose factorial is being calculated, and `acc` is the accumulator that keeps track of the product of the numbers seen so far.
- ğŸ› ï¸ The function uses a tail-recursive call to calculate the factorial of `n - 1`, passing `acc * n` as the new accumulator.
- ğŸš€ When `n` reaches 0, the function returns the accumulator, which contains the factorial of the original number.
- ğŸ“Š The function uses tail recursion and is optimized by the compiler, which turns it into a loop.

## Topic 4: Algorithm design

### 4.1 Brute-force/exhaustive search

- ğŸ¤¯Â Systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement
- Often simplest to implement but not very efficient
- Impractical for all but smallest instances of a problem
- ğŸ§Â Examples:
    - Selection sort
    - Bubble sort
    - In graphs â€“ depth-first search (DFS), breadth-first search (BFS)

### 4.2 Decrease and conquer

- ğŸ¯Â Establish relationship between a problem and a smaller instance of that problem
- Exploit that relationship top down or bottom up to solve the bigger problem
- Naturally implemented using recursion
- ğŸ§Â Examples:
    - Insertion sort
    - In graphs â€“ topological sorting

### 4.3 Divide and conquer

- ğŸ”ªÂ Divide a problem into several subproblems of the same type, ideally of the same size
- Solve subproblems, typically recursively
- If needed, combine solutions
- ğŸ§Â Examples:
    - Mergesort
    - Quicksort
    - Binary tree traversal â€“ preorder, inorder, postorder
        - Visit root, its left subtree, and its right subtree

### 4.4 Transform and conquer

- ğŸŒ€Â Modify a problem to be more amenable to solution, then solve
    - Transform to a simpler/more convenient instance of the same problem â€“ instance simplification
    - Transform to a different representation of the same instance â€“ representation change
    - Transform to an instance of a different problem for which an algorithm is available â€“ problem reduction
- ğŸ§Â Examples:
    - Balanced search trees â€“ AVL trees, 2-3 trees â€“ Reduction to graph problems

### 4.5 Dynamic programming

- ğŸ¤–Â Similar to divide and conquer, solves problems by combining the solutions to subproblems
    - In divide and conquer subproblems are disjoint
    - In dynamic programming, subproblems overlap, i.e., share subsubproblems
- Solutions to those are stored, indexed, and reused
- ğŸ§Â Examples:
    - Floyd-Warshall shortest path algorithm

### 4.6 Greedy

- ğŸ˜ˆÂ Always make the choice that looks best at the moment
    - Does not always yield the most optimal solution, but often does
- ğŸ§Â Examples
    - Graphs:
        - Dijkstra â€“ find the shortest path from the source to the vertex nearest to it, then second nearest, etc.
        - Prim
        - Kruskal
    - Hill Climbing

### 4.7 Genetic Algorithms

- ğŸ§¬ Inspired by the process of natural selection and evolution
- ğŸŒ Population-based optimization technique using stochastic methods
- ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Represents solutions as individuals in a population
- ğŸ”„ Iteratively improves the population through selection, crossover (recombination), and mutation
- ğŸ¯ Typically applied to optimization and search problems
- â²ï¸ May not always find the optimal solution, but can often find near-optimal solutions in a reasonable time
- ğŸ§ Examples:
    - Traveling Salesman Problem
    - Machine learning model hyperparameter optimization
    - Feature selection in classification problems

### 4.8 **Constraint Programming**

- â—ï¸ Formulates a problem as a set of variables, domains, and constraints
- ğŸ¯ Focuses on finding a solution that satisfies all constraints while optimizing an objective function (if any)
- ğŸ“ Variables have specific domains (ranges of possible values)
- ğŸ”— Constraints define the relationships between variables and restrict the feasible solution space
- ğŸ§  Often relies on backtracking, search, and propagation techniques to find feasible solutions
- ğŸ”§ Highly expressive and flexible for solving a wide variety of combinatorial problems
- ğŸ§ Examples:
    - Scheduling problems
    - Resource allocation
    - Puzzles (e.g., Sudoku, N-queens problem)
    - Graph coloring

## Topic 5: Misc

Stuff that doesn't really fit in anywhere else but might be important.

### 5.1 Strings

- ğŸ“š Sequences of characters: Text, Genome sequences
- ğŸ§© Characters: In C (char), In Java (char, 16-bit unsigned int)
- ğŸš€ Java.lang.String: Immutable sequence of characters.
- ğŸ”’ Security: Parameters in many methods which could introduce vulnerability - security threats, eg network connection is passed a string - it could be modified to connect to a different machine, or a modified file name can be passed in etc
- ğŸ§µ Thread-safe: No need for synchronization if shared between threads - no thread can modify it (So no Concurrent Systems Logic Needed ğŸ˜„)

#### 5.1.1 String Sorting Algorithms

- ğŸ§ When to use which?
    - Insertion: Small arrays, nearly sorted
    - Quick: General purpose, tight space
    - Merge: General purpose, stable
    - 3-way quick: Large number of equal keys
    - LSD: Short fixed-length strings
    - MSD: Random strings
    - 3-way string quicksort: General purpose, long prefix matches


## Important Notice

ğŸ’¥ **THESE NOTES ARE INCOMPLETE**. They are currently missing **`LSD`, `MSD`, `Key-indexed counting`, and `anything to do with tries`**.

âœ¨ In addition to everything here, you should also go through the project requirements, since it is explicitly stated that questions regarding the algorithms discussed for the projects may be asked.

ğŸ”§ If anyone else wants to contribute and finish these up, please make a pull request. I will not be working on these anymore since I've already finished the module.
